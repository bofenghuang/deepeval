---
id: crewai
title: CrewAI
sidebar_label: CrewAI
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# CrewAI

[CrewAI](https://www.crewai.com/) is a lean, lightning-fast Python framework for creating autonomous AI agents tailored to any scenario.

## Quickstart
Confident AI provides a `instrumentator` that can trace CrewAI's execution with just a single line of code.

Install the following package:
```bash
pip install -U deepeval
```

With your Confident API key, initialize DeepEval's `instrumentator` in your agent's `main.py` script:

```python
from deepeval.integrations.crewai import instrumentator
instrumentator(api_key="<your-confident-api-key>")
```
That's it! You can now see the traces on Confident AI's **Observability**.

## Basic CrewAI Agent

Here's a basic example of a CrewAI agent with Confident AI's `instrumentator`:

```bash
pip install crewai
```

```python filename="main.py" showLineNumbers {5, 9}
import os
import time
from crewai import Task, Crew, Agent

from deepeval.integrations.crewai import instrumentator

os.environ["OPENAI_API_KEY"] = "<your-openai-api-key>"

instrumentator(api_key="<your-confident-api-key>")

# Define your agents with roles and goals
coder = Agent(
    role='Consultant',
    goal='Write clear, concise explanation.',
    backstory='An expert consultant with a keen eye for software trends.',
)
 
# Create tasks for your agents
task1 = Task(
    description="Explain the latest trends in AI.",
    expected_output="A clear and concise explanation.",
    agent=coder
)
 
# Instantiate your crew
crew = Crew(
    agents=[coder],
    tasks=[task1],
)

# Kickoff your crew
result = crew.kickoff()
print(result)

time.sleep(7) # Wait for traces to be posted to observatory
```

Run your agent:

```bash
python main.py
```
After execution, you can see the traces on Confident AI's **Observability**.

## Online Agent Evaluation in Production

To run [online evaluations](https://documentation.confident-ai.com/docs/llm-tracing/evaluations) on your CrewAI agent:

1. Replace your CrewAI `Agent` with DeepEval's CrewAI `Agent` wrapper
2. Provide `metric_collection` as an argument to DeepEval's `Agent` wrapper
3. Run your agent


:::note
The CrewAI **Agent** wrapper auto-populates the `input`, `actual_output`, `expected_output` and `tools_called` fields of [LLMTestCase](https://deepeval.com/docs/evaluation-test-cases) for each `Agent`. Therefore, you can't include metrics that require other parameters (e.g. `retrieval_context`) when creating your [metric collection](https://deepeval.com/docs/metrics-task-completion).
:::


```python filename="main.py" showLineNumbers {1, 9}
from deepeval.integrations.crewai.agent import Agent
...

# Define your agents with metric collection name
coder = Agent(
    role='Consultant',
    goal='Write clear, concise explanation.',
    backstory='An expert consultant with a keen eye for software trends.',
    metric_collection="<your-metric-collection-name>"
)

...

# Kickoff your crew
result = crew.kickoff()
time.sleep(7) # Wait for traces to be posted to observatory
```

