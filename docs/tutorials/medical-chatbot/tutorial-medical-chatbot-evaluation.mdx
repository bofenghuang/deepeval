---
id: tutorial-medical-chatbot-evaluation
title: Evaluate Multi-Turn Convos
sidebar_label: Evaluate Multi-Turn Convos
---

In the previous section, we built a chatbot that:

- Diagnosis patients
- Schedules appointments according to the diagnosis
- Retains memory throughout a conversation

To evaluate a multi-turn chatbot that does all the above, we first have to model conversations as [multi-turn interactions](/docs/evaluation-multiturn-test-cases#multi-turn-llm-interaction) in `deepeval`:

![Conversational Test Case](https://deepeval-docs.s3.amazonaws.com/docs:conversational-test-case.png)

A multi-turn "interaction" is composed of `turns`, which is the conversation itself, and any other optional parameters such as scenario, expected outcome, etc. which we will learn about later in this section. In code, a multi-turn interaction is represented by a `ConversationalTestCase`:

```python
from deepeval.test_case import ConversationalTestCase

test_case = ConversationalTestCase(
    turns=[
        Turn(role="user", content="I've a sore throat."),
        Turn(role="assistant", content="Thanks for letting me know?"),
    ]
)
```

When you evaluate multi-turn use cases, you don't just want to run evaluations on a random set of conversations. In fact, you'll want to make sure that you're running evaluations for different iterations of your chatbot on the same set of scenarios, in order to form a valid benchmark for your chatbot in order to determine whether there are regressions, etc.

## Setup Testing Environment

When evaluating multi-turn conversations, there are three primary approaches:

1. **Use Historical Conversations** - Pull conversations from your production database and run evaluations on that existing data.

2. **Generate Conversations Manually** - Prompt the model to produce conversations in real time and then run evaluations on those conversations.

3. **Simulate User Interactions** - Interact with your chatbot through simulations, and then run evaluations on the resulting conversations.

By far, option 3 is the best way to test multi-turn conversations. But we'll still go through options 1 and 2 quickly to show why they are flawed.

### Use historical data

If you have conversations stored in your database, you can convert them to `ConversationalTestCase` objects:

```python
from deepeval.test_case import ConversationalTestCase, Turn

# Example: Fetch conversations from your database
conversations = fetch_conversations_from_db()  # Your database query here

test_cases = []
for conv in conversations:
    turns = [Turn(role=msg["role"], content=msg["content"]) for msg in conv["messages"]]
    test_case = ConversationalTestCase(turns=turns)
    test_cases.append(test_case)

print(test_cases)
```

**Using historical conversations** is the quickest to run because the data already exists, but it only provides ad-hoc insights into past performance and cannot reliably evaluate how a new version will perform. Results from this approach are mostly backward-looking.

:::tip
This example assumes each conversation is a list of messages following the OpenAI-style format, where messages have a role ("user" or "assistant") and `content`. To learn what the `Turn` data model looks like, [click here.](/docs/evaluation-multiturn-test-cases#turns)
:::

### Manual prompting

To generate conversations manually, you have to create `turn`s from interacting with your chatbot and constructing a `ConversationalTestCase` once a conversation has compeleted:

```python
from deepeval.test_case import ConversationalTestCase, Turn

# Initialize test case list
test_cases = []

def start_session(chatbot: MedicalChatbot):
    turns = []
    while True:
        user_input = input("Your query: ")
        if user_input.lower() == 'exit':
            break

        # Call chatbot
        response = chatbot.agent_with_memory.invoke({"input": user_input}, config={"configurable": {"session_id": session_id}})
        # Add turns to list
        turns.append(Turn(role="user", content=user_input))
        turns.append(Turn(role="assistant", content=response["output"]))

        print("Baymax:", response["output"])

# Initialize chatbot and start session
chatbot = MedicalChatbot(model="...", system_prompt="...")
start_session(chatbot)

# Print test cases
print(test_cases)
```

In this example, we called `chatbot.agent_with_memory.invoke` from `langchain` and collected the turns as user and assistant contents. Although effective, this method is extremely time consuming and hence not the most effective.

:::note
This method is better than using historical data because it tests the current version of your system, producing forward-looking insights instead of retrospective snapshots.
:::

### User simulations

It is highly recommended to simulate turns instead, because you:

- Test against the **current version** of your system without relying on historical conversations
- Avoid **manual prompting** and can fully automate the process
- Create **consistent benchmarks**, e.g., simulating a fixed number of conversations across the same scenarios, which makes performance comparisons straightforward (more on this later)

First standardize your testing dataset by createing a list of goldens:

```python title="main.py"
from deepeval.dataset import EvaluationDataset, ConversationalGolden

goldens = [
    ConversationalGolden(scenario="User with a sore throat asking for paracetamol."),
    ConversationalGolden(scenario="Frustrated user looking to rebook their appointment."),
    ConversationalGolden(scenario="User just looking to talk to somebody.")
]

# Create dataset and optionally push to Confident AI
dataset = EvaluationDataset(goldens=goldens)
dataset.push(alias="Medical Chatbot Dataset")
```

In reality, you'll want **at least 20 goldens** to form a valid dataset.
